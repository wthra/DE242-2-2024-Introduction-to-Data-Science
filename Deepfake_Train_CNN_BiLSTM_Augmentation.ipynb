{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeb4290e-bcab-4490-8e4b-4d7607774c15",
      "metadata": {
        "id": "eeb4290e-bcab-4490-8e4b-4d7607774c15",
        "outputId": "ea3912bd-46e1-4c07-d59b-d1811503a17b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.10.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5025c5-1623-424f-9896-3c84d7c8e9b0",
      "metadata": {
        "id": "cf5025c5-1623-424f-9896-3c84d7c8e9b0",
        "outputId": "68895305-f233-47d0-b36f-e8b49da0990d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "‚úÖ Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        }
      ],
      "source": [
        "print(\"GPUs Available: \", tf.config.experimental.list_physical_devices('GPU'))\n",
        "\n",
        "# ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        print(\"Using GPU:\", gpus[0])\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ae88672-aa2f-45dc-9d1f-4aa3af302f35",
      "metadata": {
        "id": "9ae88672-aa2f-45dc-9d1f-4aa3af302f35"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29748a26-05ad-4274-a5d1-2601be54db76",
      "metadata": {
        "id": "29748a26-05ad-4274-a5d1-2601be54db76",
        "outputId": "4e387b56-71fe-440d-da2c-41daf410438a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- ‡πÑ‡∏ü‡∏•‡πå: ASVspoof2019.LA.cm.dev.trl.txt ---\n",
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Real:\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1047731.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1105538.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1125976.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1293230.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1340209.flac\n",
            "\n",
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fake:\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1008730.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1034049.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1048723.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1067573.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\LA_D_1091909.flac\n",
            "\n",
            "\n",
            "--- ‡πÑ‡∏ü‡∏•‡πå: ASVspoof2019.LA.cm.eval.trl.txt ---\n",
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Real:\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_5849185.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_4581379.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_6314733.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_3379393.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_3757378.flac\n",
            "\n",
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fake:\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_2834763.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_8877452.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_6828287.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_6977360.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\LA_E_5932896.flac\n",
            "\n",
            "\n",
            "--- ‡πÑ‡∏ü‡∏•‡πå: ASVspoof2019.LA.cm.train.trn.txt ---\n",
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Real:\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1138215.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1271820.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1272637.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1276960.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1341447.flac\n",
            "\n",
            "‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fake:\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1004644.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1056709.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1195221.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1265032.flac\n",
            "dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\LA_T_1287124.flac\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file_names = [\n",
        "    \"ASVspoof2019.LA.cm.dev.trl.txt\",\n",
        "    \"ASVspoof2019.LA.cm.eval.trl.txt\",\n",
        "    \"ASVspoof2019.LA.cm.train.trn.txt\"\n",
        "]\n",
        "\n",
        "data = {}\n",
        "\n",
        "for file_name in file_names:\n",
        "    real_data = []\n",
        "    fake_data = []\n",
        "    try:\n",
        "        with open(f\"dataset\\\\LA\\\\{file_name}\", 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if \"bonafide\" in line:\n",
        "                    real_data.append(parts)\n",
        "                elif \"spoof\" in line:\n",
        "                    fake_data.append(parts)\n",
        "        data[file_name] = {\"real\": real_data, \"fake\": fake_data}\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_name}\")\n",
        "\n",
        "# ‡πÅ‡∏¢‡∏Å‡πÄ‡∏≠‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ö .flac\n",
        "processed_data = {}\n",
        "for file_name, categorized_data in data.items():\n",
        "    processed_data[file_name] = {\"real\": [], \"fake\": []}\n",
        "    if file_name == \"ASVspoof2019.LA.cm.dev.trl.txt\":\n",
        "        DATAPATH = os.path.join(r\"dataset\\\\LA\\\\ASVspoof2019_LA_dev\\\\flac\\\\\")\n",
        "    elif file_name == \"ASVspoof2019.LA.cm.eval.trl.txt\":\n",
        "        DATAPATH = os.path.join(r\"dataset\\\\LA\\\\ASVspoof2019_LA_eval\\\\flac\\\\\")\n",
        "    elif file_name == \"ASVspoof2019.LA.cm.train.trn.txt\":\n",
        "        DATAPATH = os.path.join(r\"dataset\\\\LA\\\\ASVspoof2019_LA_train\\\\flac\\\\\")\n",
        "\n",
        "    for line_parts in categorized_data[\"real\"]:\n",
        "        if len(line_parts) > 1:\n",
        "            processed_data[file_name][\"real\"].append(DATAPATH + line_parts[1] + \".flac\")\n",
        "    for line_parts in categorized_data[\"fake\"]:\n",
        "        if len(line_parts) > 1:\n",
        "            processed_data[file_name][\"fake\"].append(DATAPATH + line_parts[1] + \".flac\")\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß\n",
        "for file_name, categorized_data in processed_data.items():\n",
        "    print(f\"--- ‡πÑ‡∏ü‡∏•‡πå: {file_name} ---\")\n",
        "    print(\"‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Real:\")\n",
        "    for i in range(min(5, len(categorized_data[\"real\"]))):\n",
        "        print(categorized_data[\"real\"][i])\n",
        "    print(\"\\n‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå Fake:\")\n",
        "    for i in range(min(5, len(categorized_data[\"fake\"]))):\n",
        "        print(categorized_data[\"fake\"][i])\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97316ee5-d480-4191-8a99-734a177411e4",
      "metadata": {
        "id": "97316ee5-d480-4191-8a99-734a177411e4",
        "outputId": "8c73f585-15e1-4e3c-cd7e-185ce462a341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2548\n",
            "7355\n",
            "2580\n"
          ]
        }
      ],
      "source": [
        "print(len(processed_data[\"ASVspoof2019.LA.cm.dev.trl.txt\"][\"real\"]))\n",
        "print(len(processed_data[\"ASVspoof2019.LA.cm.eval.trl.txt\"][\"real\"]))\n",
        "print(len(processed_data[\"ASVspoof2019.LA.cm.train.trn.txt\"][\"real\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab4cc9c8-098a-45ae-a5c8-5c6415424301",
      "metadata": {
        "id": "ab4cc9c8-098a-45ae-a5c8-5c6415424301"
      },
      "outputs": [],
      "source": [
        "from librosa.effects import time_stretch, pitch_shift\n",
        "\n",
        "def augment_audio(y, sr):\n",
        "    aug_type = random.choice(['stretch', 'pitch', 'noise'])\n",
        "\n",
        "    try:\n",
        "        if aug_type == 'stretch':\n",
        "\n",
        "            rate = random.uniform(0.8, 1.2)\n",
        "            y_stretch = time_stretch(y, rate=rate)\n",
        "\n",
        "            # ‡∏´‡∏•‡∏±‡∏á stretch ‡∏≠‡∏≤‡∏à‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏™‡∏±‡πâ‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô ‚Üí ‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ñ‡∏á‡∏ó‡∏µ‡πà\n",
        "            expected_length = int(sr * DURATION)\n",
        "            y = librosa.util.fix_length(y_stretch, size=expected_length)\n",
        "\n",
        "        elif aug_type == 'pitch':\n",
        "            n_steps = random.randint(-2, 2)\n",
        "            y = pitch_shift(y, sr=sr, n_steps=n_steps)\n",
        "\n",
        "        elif aug_type == 'noise':\n",
        "            noise = np.random.normal(0, 0.005, y.shape)\n",
        "            y = y + noise\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Augmentation error ({aug_type}): {e}\")\n",
        "\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a205990-a427-4460-a6ca-d380e3fd2445",
      "metadata": {
        "id": "2a205990-a427-4460-a6ca-d380e3fd2445",
        "outputId": "57ef0dbf-a6ba-4c40-fd5b-f7c2709ae7e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing ASVspoof2019.LA.cm.dev.trl.txt - real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2548/2548 [00:36<00:00, 70.75it/s]\n",
            "Processing ASVspoof2019.LA.cm.dev.trl.txt - fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1528/1528 [00:24<00:00, 63.08it/s]\n",
            "Processing ASVspoof2019.LA.cm.eval.trl.txt - real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7355/7355 [02:01<00:00, 60.35it/s]\n",
            "Processing ASVspoof2019.LA.cm.eval.trl.txt - fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4413/4413 [01:21<00:00, 54.25it/s]\n",
            "Processing ASVspoof2019.LA.cm.train.trn.txt - real: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2580/2580 [01:05<00:00, 39.38it/s]\n",
            "Processing ASVspoof2019.LA.cm.train.trn.txt - fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1548/1548 [00:34<00:00, 45.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Mel Spectrograms ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß: 19972\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô labels: 19972\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 512\n",
        "HOP_LENGTH = 160\n",
        "N_MELS = 128\n",
        "N_MFCC = 20\n",
        "DURATION = 5\n",
        "\n",
        "X_mel = []\n",
        "labels = []\n",
        "\n",
        "for file_name, categories in processed_data.items():\n",
        "\n",
        "    real_ids = categories[\"real\"]\n",
        "    fake_ids = categories[\"fake\"]\n",
        "\n",
        "    sample_size = len(real_ids)\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    real_sampled = np.random.choice(real_ids, size=sample_size, replace=False)\n",
        "\n",
        "    fake_sampled = np.random.choice(fake_ids, size=int(sample_size * 0.6), replace=False)\n",
        "\n",
        "    for label_type in [\"real\", \"fake\"]:\n",
        "        current_label = 1 if label_type == \"real\" else 0\n",
        "        file_ids = real_sampled if label_type == \"real\" else fake_sampled\n",
        "\n",
        "        if file_ids.size == 0:\n",
        "            continue\n",
        "\n",
        "        for file_id in tqdm(file_ids, desc=f\"Processing {file_name} - {label_type}\"):\n",
        "            file_path = os.path.join(file_id)\n",
        "            try:\n",
        "\n",
        "                # ‡πÇ‡∏´‡∏•‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á ‡πÅ‡∏•‡∏∞ pad ‡∏ñ‡πâ‡∏≤‡∏™‡∏±‡πâ‡∏ô‡∏Å‡∏ß‡πà‡∏≤ duration\n",
        "                y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "\n",
        "                # ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• augment\n",
        "                if np.random.rand() < 0.5:\n",
        "                    y = augment_audio(y, sr)\n",
        "\n",
        "                expected_length = int(SAMPLE_RATE * DURATION)\n",
        "                if len(y) < expected_length:\n",
        "                    y = np.pad(y, (0, expected_length - len(y)))\n",
        "\n",
        "                mel_spectrogram = librosa.feature.melspectrogram(\n",
        "                    y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS\n",
        "                )\n",
        "                mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "                X_mel.append(mel_spectrogram_db)\n",
        "                labels.append(current_label)\n",
        "\n",
        "            except FileNotFoundError:\n",
        "                print(f\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà {file_path}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c36ec6d9-d8e4-478f-b3ef-395be74edfc1",
      "metadata": {
        "id": "c36ec6d9-d8e4-478f-b3ef-395be74edfc1"
      },
      "outputs": [],
      "source": [
        "jl_real_paths = [os.path.join(\"dataset/JL corpus\", f) for f in os.listdir(\"dataset/JL corpus\") if f.endswith(\".wav\")]\n",
        "np.random.seed(42)\n",
        "jl_real_sample = np.random.choice(jl_real_paths, size=2400, replace=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe5d873b-ad7f-4ff5-b266-37c65b5adc60",
      "metadata": {
        "id": "fe5d873b-ad7f-4ff5-b266-37c65b5adc60",
        "outputId": "dc7bb186-0c4c-472a-e9de-52e0e03fc249"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "‡πÄ‡∏û‡∏¥‡πà‡∏° JL corpus ‡πÑ‡∏õ‡πÉ‡∏ô training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2400/2400 [00:36<00:00, 65.98it/s]\n"
          ]
        }
      ],
      "source": [
        "for path in tqdm(jl_real_sample, desc=\"‡πÄ‡∏û‡∏¥‡πà‡∏° JL corpus ‡πÑ‡∏õ‡πÉ‡∏ô training\"):\n",
        "    y, sr = librosa.load(path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "    if len(y) < int(SAMPLE_RATE * DURATION):\n",
        "        y = np.pad(y, (0, int(SAMPLE_RATE * DURATION) - len(y)))\n",
        "\n",
        "    if np.random.rand() < 0.1:\n",
        "        y = augment_audio(y, sr)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(\n",
        "                    y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
        "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "    X_mel.append(mel_spectrogram_db)\n",
        "    labels.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acb1a548-ebda-4ab7-8639-b2195fc6f464",
      "metadata": {
        "id": "acb1a548-ebda-4ab7-8639-b2195fc6f464",
        "outputId": "5a0006e5-d539-4698-ea6e-9ee19e727c1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Mel Spectrograms ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß: 22372\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô labels: 22372\n"
          ]
        }
      ],
      "source": [
        "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Mel Spectrograms ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß: {len(X_mel)}\")\n",
        "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô labels: {len(labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "684957d9-6cab-4952-90e1-9931d4817ffb",
      "metadata": {
        "id": "684957d9-6cab-4952-90e1-9931d4817ffb"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "X_mel, labels = shuffle(np.array(X_mel), np.array(labels), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "348a8340-1d1c-48ec-8305-0294e47cdd58",
      "metadata": {
        "id": "348a8340-1d1c-48ec-8305-0294e47cdd58"
      },
      "outputs": [],
      "source": [
        "np.save('X_mel.npy', X_mel)\n",
        "np.save('labels.npy', labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b9d911d-c46a-4c3c-bdc8-adc48237829a",
      "metadata": {
        "id": "9b9d911d-c46a-4c3c-bdc8-adc48237829a"
      },
      "outputs": [],
      "source": [
        "X_mel = np.load('X_mel.npy')\n",
        "labels = np.load('labels.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "074f758c-d55b-466b-ac3e-939c9a6380fe",
      "metadata": {
        "id": "074f758c-d55b-466b-ac3e-939c9a6380fe"
      },
      "outputs": [],
      "source": [
        "X_mel = np.array(X_mel)\n",
        "y = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2746618f-b67a-4bc3-b36c-0a470bf746fd",
      "metadata": {
        "id": "2746618f-b67a-4bc3-b36c-0a470bf746fd"
      },
      "outputs": [],
      "source": [
        "#‡πÅ‡∏ö‡πà‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ó‡∏£‡∏ô‡πÅ‡∏•‡∏∞‡∏ó‡∏î‡∏™‡∏≠‡∏ö\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_mel, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4b6ef99-d7e0-43f6-b299-27d0e24338f9",
      "metadata": {
        "id": "c4b6ef99-d7e0-43f6-b299-27d0e24338f9"
      },
      "outputs": [],
      "source": [
        "del X_mel\n",
        "del labels\n",
        "del y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecbed5a0-bfe2-4630-974f-ba838b537f6e",
      "metadata": {
        "id": "ecbed5a0-bfe2-4630-974f-ba838b537f6e",
        "outputId": "3514a253-f478-4cc5-ba27-275b39634302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 128, 501, 1)       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 126, 499, 64)      640       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 63, 249, 64)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 61, 247, 128)      73856     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 30, 123, 128)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 128)              0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               16512     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 91,137\n",
            "Trainable params: 91,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    layers.Reshape((input_shape[0], input_shape[1], 1)),  # (128, TimeSteps, 1)\n",
        "\n",
        "    # Convolution layers\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "\n",
        "    # Dense layers\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f928bf0b-87a8-4976-b5e6-27f7892acd3e",
      "metadata": {
        "id": "f928bf0b-87a8-4976-b5e6-27f7892acd3e"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype(np.float32)\n",
        "X_test  = X_test.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test  = y_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9c16eb-2c57-4379-afea-79a6f27d9d8a",
      "metadata": {
        "scrolled": true,
        "id": "9c9c16eb-2c57-4379-afea-79a6f27d9d8a",
        "outputId": "f3551d93-fd00-412b-b9a5-35538eae0048"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1119/1119 [==============================] - 39s 29ms/step - loss: 0.6597 - accuracy: 0.6737 - val_loss: 0.6200 - val_accuracy: 0.6780\n",
            "Epoch 2/50\n",
            "1119/1119 [==============================] - 32s 28ms/step - loss: 0.5963 - accuracy: 0.7034 - val_loss: 0.5802 - val_accuracy: 0.7053\n",
            "Epoch 3/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.5442 - accuracy: 0.7390 - val_loss: 0.5285 - val_accuracy: 0.7419\n",
            "Epoch 4/50\n",
            "1119/1119 [==============================] - 31s 27ms/step - loss: 0.5017 - accuracy: 0.7649 - val_loss: 0.4660 - val_accuracy: 0.7875\n",
            "Epoch 5/50\n",
            "1119/1119 [==============================] - 32s 29ms/step - loss: 0.4556 - accuracy: 0.7922 - val_loss: 0.4653 - val_accuracy: 0.7785\n",
            "Epoch 6/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.4265 - accuracy: 0.8078 - val_loss: 0.3672 - val_accuracy: 0.8474\n",
            "Epoch 7/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.4117 - accuracy: 0.8123 - val_loss: 0.3602 - val_accuracy: 0.8467\n",
            "Epoch 8/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.3641 - accuracy: 0.8427 - val_loss: 0.3574 - val_accuracy: 0.8478\n",
            "Epoch 9/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.3446 - accuracy: 0.8509 - val_loss: 0.2981 - val_accuracy: 0.8726\n",
            "Epoch 10/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.3215 - accuracy: 0.8624 - val_loss: 0.3053 - val_accuracy: 0.8608\n",
            "Epoch 11/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.3156 - accuracy: 0.8642 - val_loss: 0.2894 - val_accuracy: 0.8711\n",
            "Epoch 12/50\n",
            "1119/1119 [==============================] - 31s 27ms/step - loss: 0.2915 - accuracy: 0.8794 - val_loss: 0.3118 - val_accuracy: 0.8610\n",
            "Epoch 13/50\n",
            "1119/1119 [==============================] - 31s 27ms/step - loss: 0.2828 - accuracy: 0.8814 - val_loss: 0.2741 - val_accuracy: 0.8771\n",
            "Epoch 14/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.2688 - accuracy: 0.8868 - val_loss: 0.2648 - val_accuracy: 0.8898\n",
            "Epoch 15/50\n",
            "1119/1119 [==============================] - 31s 27ms/step - loss: 0.2565 - accuracy: 0.8912 - val_loss: 0.2921 - val_accuracy: 0.8682\n",
            "Epoch 16/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.2470 - accuracy: 0.8962 - val_loss: 0.2399 - val_accuracy: 0.9097\n",
            "Epoch 17/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.2331 - accuracy: 0.9036 - val_loss: 0.2082 - val_accuracy: 0.9115\n",
            "Epoch 18/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.2219 - accuracy: 0.9098 - val_loss: 0.1869 - val_accuracy: 0.9216\n",
            "Epoch 19/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.2176 - accuracy: 0.9102 - val_loss: 0.3488 - val_accuracy: 0.8487\n",
            "Epoch 20/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.2153 - accuracy: 0.9134 - val_loss: 0.2080 - val_accuracy: 0.9218\n",
            "Epoch 21/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1993 - accuracy: 0.9213 - val_loss: 0.1860 - val_accuracy: 0.9260\n",
            "Epoch 22/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1982 - accuracy: 0.9186 - val_loss: 0.2885 - val_accuracy: 0.8813\n",
            "Epoch 23/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1959 - accuracy: 0.9210 - val_loss: 0.1605 - val_accuracy: 0.9379\n",
            "Epoch 24/50\n",
            "1119/1119 [==============================] - 32s 29ms/step - loss: 0.1853 - accuracy: 0.9276 - val_loss: 0.1706 - val_accuracy: 0.9263\n",
            "Epoch 25/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1777 - accuracy: 0.9299 - val_loss: 0.1588 - val_accuracy: 0.9377\n",
            "Epoch 26/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1825 - accuracy: 0.9281 - val_loss: 0.1478 - val_accuracy: 0.9388\n",
            "Epoch 27/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1686 - accuracy: 0.9315 - val_loss: 0.1681 - val_accuracy: 0.9314\n",
            "Epoch 28/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1573 - accuracy: 0.9383 - val_loss: 0.1325 - val_accuracy: 0.9522\n",
            "Epoch 29/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1516 - accuracy: 0.9412 - val_loss: 0.1566 - val_accuracy: 0.9330\n",
            "Epoch 30/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1485 - accuracy: 0.9411 - val_loss: 0.3504 - val_accuracy: 0.8771\n",
            "Epoch 31/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1389 - accuracy: 0.9460 - val_loss: 0.1363 - val_accuracy: 0.9477\n",
            "Epoch 32/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1411 - accuracy: 0.9438 - val_loss: 0.1643 - val_accuracy: 0.9343\n",
            "Epoch 33/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1334 - accuracy: 0.9459 - val_loss: 0.1226 - val_accuracy: 0.9506\n",
            "Epoch 34/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1431 - accuracy: 0.9454 - val_loss: 0.1325 - val_accuracy: 0.9517\n",
            "Epoch 35/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1212 - accuracy: 0.9524 - val_loss: 0.1456 - val_accuracy: 0.9399\n",
            "Epoch 36/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1171 - accuracy: 0.9541 - val_loss: 0.1195 - val_accuracy: 0.9520\n",
            "Epoch 37/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1128 - accuracy: 0.9552 - val_loss: 0.1089 - val_accuracy: 0.9584\n",
            "Epoch 38/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1126 - accuracy: 0.9564 - val_loss: 0.1026 - val_accuracy: 0.9622\n",
            "Epoch 39/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1076 - accuracy: 0.9586 - val_loss: 0.1065 - val_accuracy: 0.9604\n",
            "Epoch 40/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1086 - accuracy: 0.9587 - val_loss: 0.1865 - val_accuracy: 0.9191\n",
            "Epoch 41/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1057 - accuracy: 0.9575 - val_loss: 0.1137 - val_accuracy: 0.9591\n",
            "Epoch 42/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1070 - accuracy: 0.9596 - val_loss: 0.1031 - val_accuracy: 0.9611\n",
            "Epoch 43/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.0945 - accuracy: 0.9647 - val_loss: 0.1025 - val_accuracy: 0.9631\n",
            "Epoch 44/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1006 - accuracy: 0.9596 - val_loss: 0.1012 - val_accuracy: 0.9629\n",
            "Epoch 45/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.0932 - accuracy: 0.9635 - val_loss: 0.1192 - val_accuracy: 0.9528\n",
            "Epoch 46/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.1012 - accuracy: 0.9625 - val_loss: 0.1787 - val_accuracy: 0.9305\n",
            "Epoch 47/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.0926 - accuracy: 0.9647 - val_loss: 0.1506 - val_accuracy: 0.9435\n",
            "Epoch 48/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.0950 - accuracy: 0.9629 - val_loss: 0.1173 - val_accuracy: 0.9546\n",
            "Epoch 49/50\n",
            "1119/1119 [==============================] - 31s 28ms/step - loss: 0.0849 - accuracy: 0.9689 - val_loss: 0.1324 - val_accuracy: 0.9560\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x17489dfb040>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ EarlyStopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=10000).batch(16).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "model.fit(\n",
        "    dataset,\n",
        "    epochs=50,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff2b9f56-e384-47a5-ad58-051c349f93a4",
      "metadata": {
        "id": "ff2b9f56-e384-47a5-ad58-051c349f93a4"
      },
      "outputs": [],
      "source": [
        "model.save(\"yoga_pose_model_semifinal.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50cc941-8fff-4023-b883-85a16aef472a",
      "metadata": {
        "id": "c50cc941-8fff-4023-b883-85a16aef472a"
      },
      "outputs": [],
      "source": [
        "model = load_model('yoga_pose_model_semifinal.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb273a71-786b-4944-8f08-03191e6ef1e5",
      "metadata": {
        "id": "cb273a71-786b-4944-8f08-03191e6ef1e5",
        "outputId": "f0152a9e-84ea-4f75-8629-1ccac659312f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "üîä Processing REAL voices: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:23<00:00, 85.26it/s]\n",
            "üîª Processing FAKE voices: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:10<00:00, 94.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß! ‡∏£‡∏ß‡∏° real 2000 + fake 1000 ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î 3000 ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ========== üîß CONFIG ========== #\n",
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 512\n",
        "HOP_LENGTH = 160\n",
        "N_MELS = 128\n",
        "DURATION = 5\n",
        "N_SAMPLES = 2000  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏à‡∏£‡∏¥‡∏á\n",
        "FAKE_SAMPLES = 1000  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏õ‡∏•‡∏≠‡∏°\n",
        "\n",
        "CLIPS_DIR = \"dataset/cv-corpus-21.0-2025-03-14/th/clips\"\n",
        "\n",
        "# ========== üîπ LOAD REAL (Common Voice) ========== #\n",
        "all_mp3_files = [f for f in os.listdir(CLIPS_DIR) if f.endswith(\".mp3\")]\n",
        "selected_files = np.random.choice(all_mp3_files, size=N_SAMPLES, replace=False)\n",
        "\n",
        "X_mel = []\n",
        "labels = []\n",
        "\n",
        "for filename in tqdm(selected_files, desc=\"Processing REAL voices\"):\n",
        "    file_path = os.path.join(CLIPS_DIR, filename)\n",
        "\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "        expected_len = int(SAMPLE_RATE * DURATION)\n",
        "        if len(y) < expected_len:\n",
        "            y = np.pad(y, (0, expected_len - len(y)))\n",
        "\n",
        "        mel = librosa.feature.melspectrogram(\n",
        "            y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS\n",
        "        )\n",
        "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        X_mel.append(mel_db)\n",
        "        labels.append(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in {file_path}: {e}\")\n",
        "\n",
        "# ========== üîπ LOAD FAKE ========== #\n",
        "for file_name, categories in processed_data.items():\n",
        "    if file_name != \"ASVspoof2019.LA.cm.eval.trl.txt\":\n",
        "        continue\n",
        "\n",
        "    fake_ids = categories[\"fake\"]\n",
        "    fake_sampled = np.random.choice(fake_ids, size=int(FAKE_SAMPLES), replace=False)\n",
        "\n",
        "    for file_path in tqdm(fake_sampled, desc=\"Processing FAKE voices\"):\n",
        "        try:\n",
        "            y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "            expected_length = int(SAMPLE_RATE * DURATION)\n",
        "            if len(y) < expected_length:\n",
        "                y = np.pad(y, (0, expected_length - len(y)))\n",
        "\n",
        "            mel = librosa.feature.melspectrogram(\n",
        "                y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS\n",
        "            )\n",
        "            mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "            X_mel.append(mel_db)\n",
        "            labels.append(0)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error (fake) {file_path}: {e}\")\n",
        "\n",
        "\n",
        "X_mel = np.array(X_mel)[..., np.newaxis]\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f\"‡πÄ‡∏™‡∏£‡πá‡∏à‡πÅ‡∏•‡πâ‡∏ß! ‡∏£‡∏ß‡∏° real {N_SAMPLES} + fake {FAKE_SAMPLES} ‡πÑ‡∏î‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(X_mel)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "265d79cc-5a3a-41da-8526-d0845906828d",
      "metadata": {
        "id": "265d79cc-5a3a-41da-8526-d0845906828d"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "X_mel, labels = shuffle(np.array(X_mel), np.array(labels), random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97931439-1ba3-4590-bb67-2800990aef26",
      "metadata": {
        "id": "97931439-1ba3-4590-bb67-2800990aef26",
        "outputId": "45c67e2d-8c77-4e7b-d0e9-496440ffc900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "94/94 [==============================] - 10s 60ms/step - loss: 0.1609 - accuracy: 0.9477\n",
            "Epoch 2/10\n",
            "94/94 [==============================] - 5s 48ms/step - loss: 0.0568 - accuracy: 0.9810\n",
            "Epoch 3/10\n",
            "94/94 [==============================] - 5s 49ms/step - loss: 0.0916 - accuracy: 0.9763\n",
            "Epoch 4/10\n",
            "94/94 [==============================] - 5s 48ms/step - loss: 0.0661 - accuracy: 0.9780\n",
            "Epoch 5/10\n",
            "94/94 [==============================] - 5s 48ms/step - loss: 0.0355 - accuracy: 0.9880\n",
            "Epoch 6/10\n",
            "94/94 [==============================] - 5s 48ms/step - loss: 0.0196 - accuracy: 0.9940\n",
            "Epoch 7/10\n",
            "94/94 [==============================] - 5s 48ms/step - loss: 0.0179 - accuracy: 0.9933\n",
            "Epoch 8/10\n",
            "94/94 [==============================] - 5s 48ms/step - loss: 0.0199 - accuracy: 0.9913\n",
            "Epoch 9/10\n",
            "94/94 [==============================] - 5s 49ms/step - loss: 0.0135 - accuracy: 0.9943\n",
            "Epoch 10/10\n",
            "94/94 [==============================] - 5s 48ms/step - loss: 0.0122 - accuracy: 0.9950\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1f7110d91f0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_mel, labels, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "829bd5f3-b9f5-4dbe-9341-9696b1e054b2",
      "metadata": {
        "id": "829bd5f3-b9f5-4dbe-9341-9696b1e054b2"
      },
      "outputs": [],
      "source": [
        "model.save(\"yoga_pose_model_final.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30bdb41e-db91-4206-b731-a68c28760f13",
      "metadata": {
        "id": "30bdb41e-db91-4206-b731-a68c28760f13"
      },
      "outputs": [],
      "source": [
        "model = load_model('yoga_pose_model_final.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8103ad1d-28e0-4fed-896b-e5d9dd230933",
      "metadata": {
        "id": "8103ad1d-28e0-4fed-896b-e5d9dd230933",
        "outputId": "488ceff9-039f-4361-8536-9f51f63eb6b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing REAL voices: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05<00:00, 169.79it/s]\n",
            "Processing ASVspoof2019.LA.cm.eval.trl.txt - fake: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:04<00:00, 223.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Mel Spectrograms ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (test): 2000\n",
            "‡∏à‡∏≥‡∏ô‡∏ß‡∏ô labels (test): 2000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 512\n",
        "HOP_LENGTH = 160\n",
        "N_MELS = 128\n",
        "DURATION = 5\n",
        "SAMPLE_SIZE = 1000\n",
        "\n",
        "X_mel_test = []\n",
        "labels_test = []\n",
        "\n",
        "CLIPS_DIR = \"dataset/cv-corpus-21.0-2025-03-14/th/clips\"\n",
        "\n",
        "# ========== üîπ LOAD REAL (Common Voice) ========== #\n",
        "all_mp3_files = [f for f in os.listdir(CLIPS_DIR) if f.endswith(\".mp3\")]\n",
        "selected_files = np.random.choice(all_mp3_files, size=SAMPLE_SIZE, replace=False)\n",
        "\n",
        "X_mel = []\n",
        "labels = []\n",
        "\n",
        "for filename in tqdm(selected_files, desc=\"Processing REAL voices\"):\n",
        "    file_path = os.path.join(CLIPS_DIR, filename)\n",
        "\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "        expected_len = int(SAMPLE_RATE * DURATION)\n",
        "        if len(y) < expected_len:\n",
        "            y = np.pad(y, (0, expected_len - len(y)))\n",
        "\n",
        "        mel = librosa.feature.melspectrogram(\n",
        "            y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS\n",
        "        )\n",
        "        mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "        X_mel_test.append(mel_db)\n",
        "        labels_test.append(1)  # ‚úÖ real\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in {file_path}: {e}\")\n",
        "\n",
        "\n",
        "# ========== üîπ FAKE: ‡∏à‡∏≤‡∏Å ASVspoof2019 üîπ ========== #\n",
        "for file_name, categories in processed_data.items():\n",
        "    if file_name != \"ASVspoof2019.LA.cm.eval.trl.txt\":\n",
        "        continue\n",
        "\n",
        "    fake_ids = categories[\"fake\"]\n",
        "    np.random.seed(42)\n",
        "    fake_sampled = np.random.choice(fake_ids, size=SAMPLE_SIZE, replace=False)\n",
        "\n",
        "    for file_path in tqdm(fake_sampled, desc=f\"Processing {file_name} - fake\"):\n",
        "        try:\n",
        "            y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "            expected_length = int(SAMPLE_RATE * DURATION)\n",
        "            if len(y) < expected_length:\n",
        "                y = np.pad(y, (0, expected_length - len(y)))\n",
        "\n",
        "            mel = librosa.feature.melspectrogram(\n",
        "                y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS\n",
        "            )\n",
        "            mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "\n",
        "            X_mel_test.append(mel_db)\n",
        "            labels_test.append(0)  # ‚ùå fake = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error (fake) {file_path}: {e}\")\n",
        "\n",
        "# ========== ‚úÖ ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏• ========== #\n",
        "print(f\"\\n‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Mel Spectrograms ‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡πÅ‡∏•‡πâ‡∏ß (test): {len(X_mel_test)}\")\n",
        "print(f\"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô labels (test): {len(labels_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41c9de9a-7d23-4a3b-91eb-457592ad0c38",
      "metadata": {
        "id": "41c9de9a-7d23-4a3b-91eb-457592ad0c38"
      },
      "outputs": [],
      "source": [
        "X_mel_test = np.array(X_mel_test)\n",
        "y_test = np.array(labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3bb455c-5d9f-4366-865c-8475aedecd3e",
      "metadata": {
        "id": "c3bb455c-5d9f-4366-865c-8475aedecd3e",
        "outputId": "b02b2704-00cf-46fa-a85d-f5f86ffe4735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "63/63 [==============================] - 1s 12ms/step\n"
          ]
        }
      ],
      "source": [
        "X_mel_test = np.array(X_mel_test)\n",
        "X_mel_test = X_mel_test[..., np.newaxis]\n",
        "\n",
        "y_pred = model.predict(X_mel_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b04ed7d0-5d92-4e3b-b658-273a6ab344eb",
      "metadata": {
        "id": "b04ed7d0-5d92-4e3b-b658-273a6ab344eb",
        "outputId": "072f671e-0e43-491e-c74a-6a6610a52a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1000\n",
            "           1       1.00      0.99      0.99      1000\n",
            "\n",
            "    accuracy                           0.99      2000\n",
            "   macro avg       0.99      0.99      0.99      2000\n",
            "weighted avg       0.99      0.99      0.99      2000\n",
            "\n",
            "[[997   3]\n",
            " [  8 992]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "print(confusion_matrix(y_test, y_pred_classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f94509a9-693f-4bba-b5bc-e95fcbdce386",
      "metadata": {
        "id": "f94509a9-693f-4bba-b5bc-e95fcbdce386",
        "outputId": "f7a77602-ac97-4480-9f38-0eeef3f42d66"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Fake\", \"Real\"], yticklabels=[\"Fake\", \"Real\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947f2bba-5c22-4359-942f-9cd6647a6028",
      "metadata": {
        "id": "947f2bba-5c22-4359-942f-9cd6647a6028",
        "outputId": "f1c3da1b-8174-438c-decd-ab155f428d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99      1000\n",
            "           1       1.00      0.99      0.99      1000\n",
            "\n",
            "    accuracy                           0.99      2000\n",
            "   macro avg       0.99      0.99      0.99      2000\n",
            "weighted avg       0.99      0.99      0.99      2000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, y_pred_classes)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "147ae13d-5c34-40fe-9ceb-7195c78a1680",
      "metadata": {
        "id": "147ae13d-5c34-40fe-9ceb-7195c78a1680",
        "outputId": "a66dd498-7f0a-4bba-97f1-b4c78e762a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 99.45%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred_classes)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea51b242-3cd7-4a50-bafb-280a97f23c42",
      "metadata": {
        "id": "ea51b242-3cd7-4a50-bafb-280a97f23c42",
        "outputId": "34319322-585f-4cc2-8898-7d4c3823e164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n",
            "Predicted: Real\n"
          ]
        }
      ],
      "source": [
        "# ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á\n",
        "file_path = 'dataset/voice-message.ogg'\n",
        "y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "\n",
        "# Pad ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "expected_length = int(SAMPLE_RATE * DURATION)\n",
        "if len(y) < expected_length:\n",
        "    y = np.pad(y, (0, expected_length - len(y)))\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Mel Spectrogram\n",
        "mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
        "mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "# reshape ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "mel = mel_spectrogram_db[..., np.newaxis]  # (samples, height, width, 1)\n",
        "\n",
        "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
        "y_pred_single = model.predict(np.expand_dims(mel, axis=0))\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏£‡∏∑‡∏≠ 1\n",
        "predicted_label = (y_pred_single > 0.5).astype(int)\n",
        "if predicted_label == 1:\n",
        "    print(\"Predicted: Real\")\n",
        "\n",
        "else:\n",
        "    print(\"Predicted: Fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3a08bb-1668-4595-8696-80ac6b81b216",
      "metadata": {
        "id": "5d3a08bb-1668-4595-8696-80ac6b81b216",
        "outputId": "17ad1137-7d76-491d-e89c-e0ef51d122dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Predicted: Fake\n"
          ]
        }
      ],
      "source": [
        "# ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á\n",
        "file_path = 'dataset/fake.flac'\n",
        "y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n",
        "\n",
        "# Pad ‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏´‡∏ô‡∏î\n",
        "expected_length = int(SAMPLE_RATE * DURATION)\n",
        "if len(y) < expected_length:\n",
        "    y = np.pad(y, (0, expected_length - len(y)))\n",
        "\n",
        "# ‡∏™‡∏£‡πâ‡∏≤‡∏á Mel Spectrogram\n",
        "mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=N_FFT, hop_length=HOP_LENGTH, n_mels=N_MELS)\n",
        "mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "# reshape ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
        "mel = mel_spectrogram_db[..., np.newaxis]  # (samples, height, width, 1)\n",
        "\n",
        "# ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•\n",
        "y_pred_single = model.predict(np.expand_dims(mel, axis=0))  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏°‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á batch\n",
        "\n",
        "# ‡πÅ‡∏õ‡∏•‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô 0 ‡∏´‡∏£‡∏∑‡∏≠ 1\n",
        "predicted_label = (y_pred_single > 0.5).astype(int)  # ‡πÉ‡∏ä‡πâ threshold 0.25\n",
        "if predicted_label == 1:\n",
        "    print(\"Predicted: Real\")\n",
        "else:\n",
        "    print(\"Predicted: Fake\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e67d076-6b80-4ab7-8d54-40e52094c02d",
      "metadata": {
        "id": "8e67d076-6b80-4ab7-8d54-40e52094c02d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}